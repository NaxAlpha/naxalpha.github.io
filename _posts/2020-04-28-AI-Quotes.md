---
published: false
---
## Generating Quotes using AI

Modern language models have very good text generation capabilities which we will use to create our own philospher AI bot. This bot takes list of tags as input (like `motivation`, `life` and `inspiration`) and generate quote which fits these tags.

### Dataset

For a successful experiment, we need a huge dataset of quotes and their tags. Fortunatly, there is one available out of the box [here](https://github.com/ShivaliGoel/Quotes-500K). This dataset has a CSV file with ~500k quotes.

### Experiment 1: T5

T5 is a text-to-text model with both encoder and decode. It is good choice for such problem as one can specify tags in the **input text** and expect quote in **output text**. T5 has many varients but we will use 3B size because it has maximum size we can train on Google Colab (TPUv2). 

For training I have used [offical notebook](https://colab.research.google.com/github/google-research/text-to-text-transfer-transformer/blob/master/notebooks/t5-trivia.ipynb). After removing `trivia-qa` task, I wrote custom dataset function which loads data from CSV, shuffles tags and takes first 1-7 tags randomly. And output is quote. Following is dataset function code:

```python
@tf.function
def proc_tags(tags):
    tags = tf.string_split([tags], sep=', ').values
    tags = tf.random_shuffle(tags)
    keep = tf.cast(tf.random_uniform((1, ), minval=1, maxval=7)[0], tf.int32)
    tags = tf.strings.reduce_join([tags[:keep]], separator=', ')
    return tags

def ds_fn(split, shuffle_files=False):
  del shuffle_files
  ds = tf.data.TextLineDataset(os.path.join(DATA_DIR, split + '.csv'))
  ds = ds.map(
      functools.partial(tf.io.decode_csv, record_defaults=['', '', '']),
      num_parallel_calls=tf.data.experimental.AUTOTUNE)
  ds = ds.map(lambda *ex: dict(zip(['quote', 'author', 'tags'], ex)))
  ds = ds.map(lambda ex: dict(quote=ex['quote'], tags=proc_tags(ex['tags'])))
  return ds

for ex in tfds.as_numpy(ds_fn("train").take(5)):
  print(ex)
```

Other important configuration was input and output window sizes. I have used `{"input": 16, "output": 128}` because most of the dataset was within these limits. I have trained model for 50k iterations - given batch size of 16 (global `16x8=64`) for 3B model, single epoch takes around 5k steps so 50k iterations means `~10 epochs`.

#### Analysis



